alloy:
  alloy:
    resources:
      requests:
        cpu: 1000m
        memory: 1000Mi
      limits:
        memory: 2000Mi
    stabilityLevel: "generally-available"
    extraPorts:
    - name: otel-grpc
      port: 4317
      targetPort: 4317
      protocol: TCP
    - name: otel-http
      port: 4318
      targetPort: 4318
      protocol: TCP
    configMap:
      content: |
        prometheus.operator.servicemonitors "default" {
          forward_to = [prometheus.remote_write.mimir.receiver]
          scrape {
            default_scrape_interval = "15s"
          }
        }

        prometheus.operator.podmonitors "default" {
          forward_to = [prometheus.remote_write.mimir.receiver]
          scrape {
            default_scrape_interval = "15s"
          }
        }

        prometheus.operator.probes "default" {
          forward_to = [prometheus.remote_write.mimir.receiver]
          scrape {
            default_scrape_interval = "15s"
          }
        }

        mimir.rules.kubernetes "default" {
          address = "http://mimir-gateway.mimir.svc.cluster.local:80"
          tenant_id = "ben"
        }

        otelcol.receiver.otlp "default" {
          grpc {
            endpoint = "0.0.0.0:4317"
          }
          http {
            endpoint = "0.0.0.0:4318"
          }
          output {
            metrics = [otelcol.processor.k8sattributes.default.input]
            logs    = [otelcol.processor.k8sattributes.default.input]
            traces  = [otelcol.processor.k8sattributes.default.input,otelcol.processor.transform.default.input]
          }
        }

        otelcol.processor.k8sattributes "default" {
          pod_association {
            source {
              from = "resource_attribute"
              name = "k8s.pod.ip"
            }
          }
          output {
            metrics = [otelcol.processor.batch.default.input]
            logs    = [otelcol.processor.batch.default.input]
            traces  = [otelcol.processor.batch.default.input]
          }
        }

        // Remove all resource attributes except the ones which
        // the otelcol.connector.spanmetrics needs.
        // If this is not done, otelcol.exporter.prometheus may fail to
        // write some samples due to an "err-mimir-sample-duplicate-timestamp" error.
        // This is because the spanmetricsconnector will create a new
        // metrics resource scope for each traces resource scope.
        otelcol.processor.transform "default" {
          error_mode = "ignore"
          trace_statements {
            context = "resource"
            statements = [
              // We keep only the "service.name" and "special.attr" resource attributes,
              // because they are the only ones which otelcol.connector.spanmetrics needs.
              //
              // There is no need to list "span.name", "span.kind", and "status.code"
              // here because they are properties of the span (and not resource attributes):
              // https://github.com/open-telemetry/opentelemetry-proto/blob/v1.0.0/opentelemetry/proto/trace/v1/trace.proto
              `keep_keys(attributes, ["service.name", "special.attr"])`,
            ]
          }
          output {
            traces  = [otelcol.connector.spanmetrics.default.input,otelcol.connector.servicegraph.default.input]
          }
        }

        otelcol.connector.spanmetrics "default" {
          metrics_flush_interval = "15s"
          dimension {
            name = "http.status_code"
          }
          dimension {
            name = "http.method"
          }
          histogram {
            explicit {}
          }
          exemplars {
            enabled = true
          }
          output {
            metrics = [otelcol.processor.batch.default.input]
          }
        }

        otelcol.connector.servicegraph "default" {
          metrics_flush_interval = "15s"
          dimensions = ["http.method"]
          output {
            metrics = [otelcol.processor.batch.default.input]
          }
        }

        otelcol.processor.batch "default" {
          output {
            metrics = [otelcol.exporter.prometheus.default.input]
            logs    = [otelcol.exporter.loki.default.input]
            traces  = [otelcol.exporter.otlp.local_tempo.input]
          }
        }

        otelcol.exporter.prometheus "default" {
          forward_to = [prometheus.remote_write.mimir.receiver]
        }

        otelcol.exporter.loki "default" {
          forward_to = [loki.write.local.receiver]
        }

        prometheus.remote_write "mimir" {
          endpoint {
            url = "http://mimir-gateway.mimir.svc.cluster.local/api/v1/push"
            headers = {
              "X-Scope-OrgID" = "ben",
            }
          }
        }

        loki.write "local" {
          endpoint {
            url = "http://loki-gateway.loki.svc.cluster.local:80/loki/api/v1/push"
            headers = {
              "X-Scope-OrgID" = "ben",
            }
          }
        }

        otelcol.exporter.otlp "local_tempo" {
          client {
            endpoint = "tempo-distributor.tempo.svc.cluster.local:4317"
            headers = {
              "X-Scope-OrgID" = "ben",
            }
            tls {
              insecure = true
              insecure_skip_verify = true
            }
          }
        }

  controller:
    type: statefulset
  serviceMonitor:
    enabled: true
